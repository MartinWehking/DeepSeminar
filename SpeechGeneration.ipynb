{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hansard.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzZTiNFHFQYZ",
        "colab_type": "text"
      },
      "source": [
        "# Hansard Text Generation\n",
        "\n",
        "**Goal:** \\\\\n",
        "We want to generate texts for partys with fewer speeches in our small dataset.\n",
        "This way we could augment our dataset so that we can use our text generator to generate speeches with positive and negative stances towards motions. Therefore we want to make use of **transfer learning** in order to train our generators on a bigger dataset and use the weights later to continue training on the smaller one.\n",
        "\n",
        "**Update** \\\\\n",
        "Unfortunately, there was not enough time to do transfer learning. Although, we were able to generate speeches and compare different language models for that task. \n",
        "\n",
        "\n",
        "First things first: We import all packages that we need for our project. \\\\\n",
        "**Notice** \\\\\n",
        " The imports include *Google Colab* imports.\n",
        "On a 'normal' system these lines can simply be removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aDFYOCnI2ht",
        "colab_type": "code",
        "outputId": "11f99cde-f967-4705-a3fd-107443311042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import time\n",
        "import nltk\n",
        "import warnings\n",
        "import re\n",
        "import nltk\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from tensorflow.keras.models import Model\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc, make_scorer, precision_score, recall_score, accuracy_score \n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, Embedding, concatenate, Conv1D, MaxPooling1D, LSTM, Bidirectional\n",
        "\n",
        "\n",
        "# Only needed if Google Colab is used\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIJo5CrHJsvf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Preparing the [Data Set](https://evanodell.com/projects/datasets/hansard-data/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pu-MPfLp4mu",
        "colab_type": "text"
      },
      "source": [
        "The dataset contains speeches in the **House of Commons** from 1979 - 2018.\n",
        "[Hansard](https://hansard.parliament.uk/) is the official report of all parliamentary debates. Speeches, bills etc. can be simply retrieved from the website. The dataset which we use was created and made public by Evan Odell in 2019. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY8xs9V8JvKR",
        "colab_type": "code",
        "outputId": "7c0e16ca-cab6-461d-c869-f371786c4d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset =  pd.read_csv(\"gdrive/My Drive/Hansard/data/hansard7918.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2,7,9,22,23,24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVZbsxuTKaKS",
        "colab_type": "text"
      },
      "source": [
        "We only take speeches from **1997** onwards and for simplicity, speeches from the **Scottish National Party**. Some brackets randomly occured in the dataset. We simply removed those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0HZr4CfKbk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only speeches from 1997 onwards\n",
        "sub_dataset = dataset[(dataset['party_group'] == 'SNP') & (dataset['year'] >= 1997)]\n",
        "# Remove new lines\n",
        "#sub_set = sub_dataset['speech'].str.replace(regex, '')\n",
        "\n",
        "# hon -> honorable due to sentence segmentation later\n",
        "sub_set = sub_dataset['speech'].str.replace('hon.', 'honorable')\n",
        "\n",
        "# Remove random brackets\n",
        "sub_set = sub_set.str.replace(')', '')\n",
        "sub_set = sub_set.str.replace('(', '')\n",
        "# Add new lines after each speech\n",
        "speeches = sub_set.astype(str) + '\\n'\n",
        "\n",
        "text = \"\".join(speeches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4a4niKuTTxD",
        "colab_type": "text"
      },
      "source": [
        "### Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWTOgHH_TZco",
        "colab_type": "code",
        "outputId": "85431877-efae-407b-a910-3539f5d37fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist(word.lower() for word in word_tokenize(text))\n",
        "fdist.plot(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEVCAYAAAAsHqjeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5rzAkRFRrPY",
        "colab_type": "code",
        "outputId": "3ad3657a-1a80-4117-db67-459a215b1024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Number of words: \"+str(len(fdist)))\n",
        "print(\"Number of characters: \"+str(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: 64436\n",
            "Number of characters: 22553797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh9CIO-XTb-f",
        "colab_type": "text"
      },
      "source": [
        "# Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jExVLRUYsgXJ",
        "colab_type": "text"
      },
      "source": [
        "We create Language Models in order to generate speeches. A language model has the advantage that it is trained to learn a conditional probability distribuition of characters/words (We refer to both as 'token') based on the frequency in the dataset. It hereby decides which token to output by the previous tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Anir94LZ0cO",
        "colab_type": "text"
      },
      "source": [
        "### Loss and Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viqdxy8Vncqd",
        "colab_type": "text"
      },
      "source": [
        "We calculate loss and perplexity on our test set after training.\n",
        "Perplexity calculation is simple in the end. \\\\\n",
        "The equation can be found [here](https://stackoverflow.com/questions/41881308/how-to-calculate-perplexity-of-rnn-in-tensorflow) for instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3pssvd9ZzEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_final_loss(model, data):\n",
        "  print('Testing:')\n",
        "  out = model.evaluate(data)\n",
        "  train_perplexity = tf.exp(out)\n",
        "  print('Test Loss: '+ str(out))\n",
        "  print('Perplexity: '+str(train_perplexity.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8C188qGMapP",
        "colab_type": "text"
      },
      "source": [
        "## Character Based Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJgfgHrrujr",
        "colab_type": "text"
      },
      "source": [
        "A character based language model receives as input a sequence of characters and tries to predict the next following characters.\n",
        "Our data is mapped to ID's which we use to identify specific characters. Each character receives its own unique ID. This makes it possible to perform computations with our character based language models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTsUcH-MnH9",
        "colab_type": "code",
        "outputId": "c8211f52-bffe-44df-9c85-7ca9642cdcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "char_vocab = set(text)\n",
        "\n",
        "char2idx = {u: i for i, u in enumerate(char_vocab)}\n",
        "idx2char = np.array(list(char_vocab))\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'h', 'k', 'K', 'ò', 's', '3', 'Ö', 'Ó', '/', '\\xa0', '0', 'F', 'ù', '7', 'x', '@', '&', 'O', 'M', '—', 'ä', 'ë', 'u', '…', 'w', 'ø', '\"', '°', '4', 'm', '’', 'd', 'J', 'E', 'C', 'ï', ',', 'Ù', '9', 'ç', ' ', 'ó', 'ć', 'ž', 'r', 'o', 'g', 'z', 'Z', 'v', 'q', 'ê', '#', 'É', 't', 'U', '.', 'D', 'R', \"'\", '1', 'ô', 'S', 'p', 'è', 'y', 'Q', '“', ':', 'í', '£', '-', 'N', 'j', 'á', 'T', 'Y', '5', 'e', 'ã', 'G', '$', '€', '\\u200b', '”', 'b', '*', '–', '‘', ';', '8', 'A', 'I', '%', 'î', 'a', 'à', '?', 'L', '\\n', 'P', 'X', 'ö', 'ü', 'c', 'i', '2', 'š', 'é', '6', '\\\\', 'B', 'W', '!', '+', ']', 'â', 'H', 'V', 'n', 'l', '`', '_', 'f'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZakHiyESelU",
        "colab_type": "text"
      },
      "source": [
        "The dataset preparation for our character based models is done as follows: We create input sequences that consist of all characters except the last one. Similarly the target sentences consist of all characters except the first one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFN7Pecpw0nC",
        "colab_type": "text"
      },
      "source": [
        "### Train and Test Set Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3IznQZ2M9Jv",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameters:**  \n",
        "\n",
        "*   Batch size: *Number of elements in one iteration (number of rows in matrix)*\n",
        "*   Buffer size: *Parameter for shuffeling the data (needed for TF2)*\n",
        "*   Train_Test_Split: *The number of samples (in %) from the dataset used for training. The rest is used during testing*\n",
        "*   Sequence length: *Number of tokens fed into the model (one sample)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ5CiX32xHNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 2048\n",
        "BUFFER_SIZE = 10000\n",
        "seq_length = 70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO14kRWASfIv",
        "colab_type": "code",
        "outputId": "e4b4f04e-d664-48bf-eed3-326951e98807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "examples_per_epoch = len(text)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "len_dataset = tf.data.experimental.cardinality(dataset)\n",
        "train_test_split = int(3 * len_dataset/4)\n",
        "\n",
        "train_dataset = dataset.take(train_test_split)\n",
        "test_dataset = dataset.skip(train_test_split)\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, \n",
        "                                                         drop_remainder=True)\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,\n",
        "                                                       drop_remainder=True)\n",
        "\n",
        "print ('Total Characters: ' + str(len(text)))\n",
        "print (\"Total Vocab: \"+str(len(char_vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 22553797\n",
            "Total Vocab: 124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5WywzKCRXeG",
        "colab_type": "text"
      },
      "source": [
        "### Text Generation Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdkIwjHz0Ilf",
        "colab_type": "text"
      },
      "source": [
        "Our text generation method takes as input a string and generates characters based on that string. As the method is the same for all character based language models, we define it beforehand. Hence, we can reuse simply reuse it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6b2kHl03cM",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameters:**\n",
        "*   Num_Generate: *Tokens to generate*\n",
        "*   Temperature: *Low temperature leads to more predictable words. High temperature to more surprising text.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu6UqixtcMmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_char_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 600\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.8\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the word returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # We pass the predicted word as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ND4Abx_IXFO",
        "colab_type": "text"
      },
      "source": [
        "### First Model: GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Y_Q_ngU2t4",
        "colab_type": "text"
      },
      "source": [
        "Eventually we can build our first model which is equivalent to the model [here](https://www.tensorflow.org/tutorials/text/text_generation). Our architecture consists of an embedding layer, GRU units and a final dense layer. The number of RNN units and our embedding dimension can be manually specified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7w4X0PoaJpy",
        "colab_type": "text"
      },
      "source": [
        "As our training process needs a lot of time, we create checkpoints that we can use as a kind of backup if something goes wrong"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3mlMebzaKJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints for the GRU language model will be saved\n",
        "gru_checkpoint_dir = 'gdrive/My Drive/Hansard/GRU/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "gru_checkpoint_prefix = os.path.join(gru_checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "gru_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=gru_checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6aRKRiNNFt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "rnn_units = 1024\n",
        "embedding_dim = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmTkKyhFIOdY",
        "colab_type": "code",
        "outputId": "d4054546-cd2a-411a-eaca-140df03fa8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "def build_gru_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        " \n",
        "model = build_gru_model(\n",
        "    vocab_size=len(char_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "EPOCHS = 10\n",
        "history = model.fit(train_dataset, epochs=EPOCHS, callbacks=[gru_checkpoint_callback])\n",
        "\n",
        "get_final_loss(model, test_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 116 steps\n",
            "Epoch 1/10\n",
            "116/116 [==============================] - 75s 646ms/step - loss: 2.7851\n",
            "Epoch 2/10\n",
            "116/116 [==============================] - 68s 589ms/step - loss: 2.0468\n",
            "Epoch 3/10\n",
            "116/116 [==============================] - 68s 588ms/step - loss: 1.6581\n",
            "Epoch 4/10\n",
            "116/116 [==============================] - 69s 599ms/step - loss: 1.4051\n",
            "Epoch 5/10\n",
            "116/116 [==============================] - 68s 589ms/step - loss: 1.2798\n",
            "Epoch 6/10\n",
            "116/116 [==============================] - 68s 588ms/step - loss: 1.2116\n",
            "Epoch 7/10\n",
            "116/116 [==============================] - 68s 590ms/step - loss: 1.1684\n",
            "Epoch 8/10\n",
            "116/116 [==============================] - 68s 589ms/step - loss: 1.1375\n",
            "Epoch 9/10\n",
            "116/116 [==============================] - 68s 590ms/step - loss: 1.1147\n",
            "Epoch 10/10\n",
            "116/116 [==============================] - 68s 588ms/step - loss: 1.0963\n",
            "Testing:\n",
            "38/38 [==============================] - 35s 931ms/step - loss: 1.1211\n",
            "Test Loss: 1.121060798042699\n",
            "Perplexity: 3.0681071196716365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTGtW62WbTPV",
        "colab_type": "text"
      },
      "source": [
        "### Second Model: Stacked LSTM\n",
        "Our second model is inspired by the architecture [here]( https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/). A stacked LSTM with dropout. The model does not yield the results that we exspected. Our first model performed better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPLkDRwySxQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints for the GRU language model will be saved\n",
        "lstm_checkpoint_dir = 'gdrive/My Drive/Hansard/LSTM/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "lstm_checkpoint_prefix = os.path.join(lstm_checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "lstm_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=lstm_checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJNb0OvBba37",
        "colab_type": "code",
        "outputId": "56d7d690-a318-4073-f6a2-68b75b4deb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "# Hyperparameters\n",
        "lstm_units = 256\n",
        "embedding_dim = 256\n",
        "\n",
        "def build_LSTM(vocab_size, embedding_dim, lstm_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "              tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                        batch_input_shape=[batch_size, None]),  \n",
        "              tf.keras.layers.LSTM(lstm_units,\n",
        "                                   return_sequences=True,\n",
        "                                   stateful=True,\n",
        "                                   recurrent_dropout=0.01),\n",
        "              tf.keras.layers.LSTM(lstm_units,\n",
        "                                   return_sequences=True,\n",
        "                                   stateful=True,\n",
        "                                   recurrent_dropout=0.01),\n",
        "              tf.keras.layers.Dense(vocab_size)\n",
        "              ])\n",
        "    return model\n",
        "  \n",
        "model = build_LSTM(\n",
        "    vocab_size=len(char_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    lstm_units=lstm_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "EPOCHS = 10\n",
        "history = model.fit(train_dataset, epochs=EPOCHS, callbacks=[lstm_checkpoint_callback])\n",
        "get_final_loss(model, test_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 116 steps\n",
            "Epoch 1/10\n",
            "116/116 [==============================] - 52s 444ms/step - loss: 3.1124\n",
            "Epoch 2/10\n",
            "116/116 [==============================] - 48s 412ms/step - loss: 2.4395\n",
            "Epoch 3/10\n",
            "116/116 [==============================] - 48s 412ms/step - loss: 2.1258\n",
            "Epoch 4/10\n",
            "116/116 [==============================] - 48s 417ms/step - loss: 1.9037\n",
            "Epoch 5/10\n",
            "116/116 [==============================] - 48s 414ms/step - loss: 1.7479\n",
            "Epoch 6/10\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 1.6334\n",
            "Epoch 7/10\n",
            "116/116 [==============================] - 48s 415ms/step - loss: 1.5498\n",
            "Epoch 8/10\n",
            "116/116 [==============================] - 48s 411ms/step - loss: 1.4886\n",
            "Epoch 9/10\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 1.4425\n",
            "Epoch 10/10\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 1.4066\n",
            "Testing:\n",
            "38/38 [==============================] - 36s 945ms/step - loss: 1.4001\n",
            "Test Loss: 1.4001051438482184\n",
            "Perplexity: 4.055626368590851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzP9XmJ-cJ-s",
        "colab_type": "text"
      },
      "source": [
        "## Char Model Language Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5iH8G1sbceL",
        "colab_type": "text"
      },
      "source": [
        "We reload our models and generate speeches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77dYPf6cb6ME",
        "colab_type": "code",
        "outputId": "cb44f539-320e-4768-dfe9-4fc67c60ef14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "tf.train.latest_checkpoint(lstm_checkpoint_dir)\n",
        "\n",
        "lstm_model = build_LSTM(len(char_vocab), embedding_dim, lstm_units, batch_size=1)\n",
        "\n",
        "lstm_model.load_weights(tf.train.latest_checkpoint(lstm_checkpoint_dir))\n",
        "\n",
        "lstm_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            31744     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 256)            525312    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (1, None, 256)            525312    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 124)            31868     \n",
            "=================================================================\n",
            "Total params: 1,114,236\n",
            "Trainable params: 1,114,236\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guQQn1tbl35H",
        "colab_type": "code",
        "outputId": "ee9752e3-1b65-4a20-ec81-a556659dfd1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_char_text(lstm_model, start_string=\"Boris Johnson\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boris Johnson dis Prime Bill asso but an and A Scottish Government. The loog in Speadhing in pecolatics words to look a preme to plause can eptent—and the same many of the decision end nated for the continue the f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38vNqx1DmB41",
        "colab_type": "code",
        "outputId": "db6f06dd-d54c-4a53-f85e-3206014b20d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "tf.train.latest_checkpoint(gru_checkpoint_dir)\n",
        "\n",
        "gru_model = build_gru_model(len(char_vocab), embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "gru_model.load_weights(tf.train.latest_checkpoint(gru_checkpoint_dir))\n",
        "\n",
        "gru_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            31744     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 124)            127100    \n",
            "=================================================================\n",
            "Total params: 4,097,148\n",
            "Trainable params: 4,097,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT3Ad7b5mq3c",
        "colab_type": "code",
        "outputId": "5ed899a3-307a-43d2-f58d-d847eab9f6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_char_text(gru_model, start_string=\"Boris Johnson\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boris Johnson, who is a pleasure to say that it is in the shared with the experience from the House and the devastating vote so on. I was systematically have saw in order to be fair the risk upstanding of parliame\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NKBzB3uiKnc",
        "colab_type": "text"
      },
      "source": [
        "## Word - Level Based Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku7QqT85HzYc",
        "colab_type": "text"
      },
      "source": [
        "We decided to not just use character based language models but also word based ones. An example with explanation can be found [here](https://medium.com/towards-artificial-intelligence/sentence-prediction-using-word-level-lstm-text-generator-language-modeling-using-rnn-a80c4cda5b40) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkK9g_i_fj4b",
        "colab_type": "code",
        "outputId": "9ec65fd5-237b-45b1-c5fb-7ee10ca439e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences = sent_detector.tokenize(text.strip())\n",
        "def strip_dot(text):\n",
        "  import string\n",
        "  if text[-1] in string.punctuation:\n",
        "    return text[:-1]\n",
        "  else:\n",
        "    return text\n",
        "eos = map(strip_dot, sentences)\n",
        "processed_text = \" <EOS> \".join(eos)\n",
        "processed_text = processed_text.replace(',',' ')\n",
        "words = processed_text.split()\n",
        "\n",
        "word_vocab = set(words)\n",
        "\n",
        "print(len(word_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfjH9JVCauOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx = dict([(word, id) for id, word in enumerate(iter(word_vocab))])\n",
        "idx2word = np.array(list(word_vocab))\n",
        "\n",
        "text_as_int_w = np.array([word2idx[c] for c in words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SpRJQiMxCtJC"
      },
      "source": [
        "**Hyperparameters:**  \n",
        "\n",
        "*   Batch size: *Number of elements in one iteration (number of rows in matrix)*\n",
        "*   Buffer size: *Parameter for shuffeling the data (needed for TF2)*\n",
        "*   Train_Test_Split: *The number of samples (in %) from the dataset used for training. The rest is used during testing*\n",
        "*   Sequence length: *Number of tokens fed into the model (one sample)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF3BiOOtCrQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNw7BqE3fwju",
        "colab_type": "code",
        "outputId": "066ba050-9f37-47ce-d3cf-c0b705ed5a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "seq_length = 10\n",
        "examples_per_epoch = len(words)\n",
        "\n",
        "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int_w)\n",
        "\n",
        "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "word_dataset = sequences.map(split_input_target)\n",
        "\n",
        "len_dataset = tf.data.experimental.cardinality(word_dataset)\n",
        "train_test_split = int(3 * len_dataset/4)\n",
        "\n",
        "train_dataset = word_dataset.take(train_test_split)\n",
        "test_dataset = word_dataset.skip(train_test_split)\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "vocab_size = len(word_vocab)\n",
        "\n",
        "print ('Total Words: ' + str(len(words)))\n",
        "print (\"Total Vocab: \"+str(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Words: 3969581\n",
            "Total Vocab: 83882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPWNbjuuftiX",
        "colab_type": "text"
      },
      "source": [
        "### GRU Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD3xqkKtDGRS",
        "colab_type": "text"
      },
      "source": [
        "Same model as the GRU model used for the character based language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26AzkVlwXJXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints for the GRU language model will be saved\n",
        "gru_word_checkpoint_dir = 'gdrive/My Drive/Hansard/Word/GRU/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "gru_word_checkpoint_prefix = os.path.join(gru_word_checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "gru_word_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=gru_word_checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04KpmZU5nBgx",
        "colab_type": "code",
        "outputId": "a3957f3c-e6c7-496b-cb62-356778dc33fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "# Hyperparameters\n",
        "rnn_units = 1024\n",
        "embedding_dim = 256\n",
        "EPOCHS = 5\n",
        "\n",
        "def build_word_gru(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        " \n",
        "model = build_word_gru(\n",
        "    vocab_size=len(word_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(train_dataset, epochs=EPOCHS, callbacks=[gru_word_checkpoint_callback])\n",
        "\n",
        "get_final_loss(model, test_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 264 steps\n",
            "Epoch 1/5\n",
            "264/264 [==============================] - 257s 973ms/step - loss: 7.2230\n",
            "Epoch 2/5\n",
            "264/264 [==============================] - 260s 986ms/step - loss: 7.0511\n",
            "Epoch 3/5\n",
            "264/264 [==============================] - 256s 968ms/step - loss: 7.0606\n",
            "Epoch 4/5\n",
            "264/264 [==============================] - 256s 971ms/step - loss: 7.0618\n",
            "Epoch 5/5\n",
            "264/264 [==============================] - 260s 986ms/step - loss: 7.0635\n",
            "Testing:\n",
            "88/88 [==============================] - 41s 464ms/step - loss: 7.2746\n",
            "Test Loss: 7.274604656479576\n",
            "Perplexity: 1443.180527280871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEO-MGdLt42z",
        "colab_type": "code",
        "outputId": "7de20570-1f77-440d-a785-1587cf8abe3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "tf.train.latest_checkpoint(gru_word_checkpoint_dir)\n",
        "\n",
        "word_model = build_word_gru(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "word_model.load_weights(tf.train.latest_checkpoint(gru_word_checkpoint_dir))\n",
        "\n",
        "word_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "word_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (1, None, 256)            21473792  \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (1, None, 1024)           6297600   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (1, None, 83882)          85979050  \n",
            "=================================================================\n",
            "Total params: 117,688,746\n",
            "Trainable params: 117,688,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJuabpe8DV2z",
        "colab_type": "text"
      },
      "source": [
        "Our generate text method now generates words instead of characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS8hxa7Ifh_I",
        "colab_type": "code",
        "outputId": "4743119f-91ad-42d6-8357-f85b6f2af861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 150\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [word2idx[s] for s in start_string.split()]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.8\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the word returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # We pass the predicted word as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "    return (start_string +' ' + ' '.join(text_generated))\n",
        "\n",
        "\n",
        "print(generate_text(word_model, start_string=\"Boris Johnson\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boris Johnson veterans could in a in criminal represents the of become the in hoping <EOS> also the get that of the the and industry our and lot very is simple it <EOS> because <EOS> has urge the in in the of the our voted I my of from deliver held tortured rethink about how of of Government and of in \"“not the for million—as the in of Secretary are of and and who the of the in domestic they the we the is and about 10 honorable in very in a the the in housing the the the been the the the <EOS> <EOS> again.\\n\" that appalling.\\n\" this the of to he of to is It the <EOS> and our the was <EOS> widespread of need However the the I the <EOS> their it but blanket of to of requests and some Government be The and take we the right the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxHw2dz9zmYc",
        "colab_type": "text"
      },
      "source": [
        "##  Word Char Combination\n",
        "\n",
        "We attempted to use a combinated [version](https://arxiv.org/pdf/1606.01700) of character and word based language model. Unfortunately this did not work out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgARON0PFq-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dim = 128\n",
        "char_dim = 128\n",
        "lstm_units = 128\n",
        "\n",
        "def build_comb_model(word_dim, char_dim, lstm_units, batch_size):\n",
        "  in_word = tf.keras.layers.Input(batch_size, None)\n",
        "  in_char = tf.keras.layers.Input(batch_size, None)\n",
        "  emb_word = tf.keras.layers.Embedding(len(word_vocab), word_dim) (in_word)\n",
        "  emb_char = tf.keras.layers.Embedding(len(char_vocab), char_dim) (in_char)\n",
        "  lstm_w = tf.keras.layers.LSTM(lstm_units)(emb_word)\n",
        "  lstm_c = tf.keras.layers.LSTM(lstm_units)(emb_char)\n",
        "  conc = tf.keras.layers.concatenate([lstm_w, lstm_c])\n",
        "  emb_c = tf.keras.layers.Embedding(2 * lstm_units, 128)(conc)\n",
        "  co_lstm = tf.keras.layers.LSTM(lstm_units)(emb_c)\n",
        "  dense = tf.keras.layers.Dense(word_vocab, activation='softmax')(co_lstm)\n",
        "  model = tf.keras.Model(inputs=[in_word, in_char], outputs=[dense])\n",
        "  return model\n",
        "\n",
        "model = build_comb_model(word_dim, char_dim, lstm_units, BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ak0kxyuHEh0",
        "colab_type": "text"
      },
      "source": [
        "# Short Version - Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiydrCW5Heyv",
        "colab_type": "text"
      },
      "source": [
        "We use 2 types of language models:\n",
        "\n",
        "\n",
        "1.   Character based language models\n",
        "2.   Word based language models\n",
        "\n",
        "Each of those has very different vocabulary sizes \\\\\n",
        "**The character-based models:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HIAO4qdLHf7U",
        "outputId": "49c50800-fd29-45d0-b1fa-00b3f1f20780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print ('Total Characters: ' + str(len(text)))\n",
        "print (\"Total Vocab: \"+str(len(char_vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 22553797\n",
            "Total Vocab: 124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV18903eIePZ",
        "colab_type": "text"
      },
      "source": [
        "**The word-based models:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUronY5YHMUN",
        "colab_type": "code",
        "outputId": "5cce81fb-792d-41cc-e192-6c18949e1e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print ('Total Words: ' + str(len(words)))\n",
        "print (\"Total Vocab: \"+str(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Words: 3969581\n",
            "Total Vocab: 83882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92ZS1tJyIpVg",
        "colab_type": "text"
      },
      "source": [
        "We applied several kinds of **preprocessing** for each kind of language model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNPlzcR_I7ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only speeches from 1997 onwards\n",
        "sub_dataset = dataset[(dataset['party_group'] == 'SNP') & (dataset['year'] >= 1997)]\n",
        "\n",
        "# hon -> honorable due to sentence segmentation later\n",
        "sub_set = sub_dataset['speech'].str.replace('hon.', 'honorable')\n",
        "\n",
        "# Remove random brackets\n",
        "sub_set = sub_set.str.replace(')', '')\n",
        "sub_set = sub_set.str.replace('(', '')\n",
        "# Add new lines after each speech\n",
        "speeches = sub_set.astype(str) + '\\n'\n",
        "\n",
        "###### For word based models only ######\n",
        "# Remove punctuations\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences = sent_detector.tokenize(text.strip())\n",
        "def strip_dot(text):\n",
        "  import string\n",
        "  if text[-1] in string.punctuation:\n",
        "    return text[:-1]\n",
        "  else:\n",
        "    return text\n",
        "eos = map(strip_dot, sentences)\n",
        "# Add EOS token at the end\n",
        "processed_text = \" <EOS> \".join(eos)\n",
        "processed_text = processed_text.replace(',',' ')\n",
        "words = processed_text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5xytSYbJqfw",
        "colab_type": "text"
      },
      "source": [
        "We used 2 different architectures for character based language models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HGhtPLwJzOT",
        "colab_type": "code",
        "outputId": "41de8aff-0e77-403c-bfbc-a2ff8c94bcea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            31744     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 124)            127100    \n",
            "=================================================================\n",
            "Total params: 4,097,148\n",
            "Trainable params: 4,097,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au9vONDDJ7sd",
        "colab_type": "code",
        "outputId": "bae7b59d-bf47-4ccd-fc7b-68044eb0e9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            31744     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 256)            525312    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (1, None, 256)            525312    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 124)            31868     \n",
            "=================================================================\n",
            "Total params: 1,114,236\n",
            "Trainable params: 1,114,236\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt_VL4G9J-yQ",
        "colab_type": "text"
      },
      "source": [
        "For language models we used just one architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDc0Bkv_KKkh",
        "colab_type": "code",
        "outputId": "2e5eb706-adee-4b71-ae09-ccf1ea2068d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "word_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (1, None, 256)            21473792  \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (1, None, 1024)           6297600   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (1, None, 83882)          85979050  \n",
            "=================================================================\n",
            "Total params: 117,688,746\n",
            "Trainable params: 117,688,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mN8_h4EKsM0",
        "colab_type": "text"
      },
      "source": [
        "The fun part: **Language generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FlP04TYKn7t",
        "colab_type": "code",
        "outputId": "127a98d4-21c1-4162-d09b-c0be7f766805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(generate_char_text(gru_model, start_string=\"Boris Johnson\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boris Johnson and I will do that crime and its urge the service personnel access to be made the extent, but the problems in itself. We look at the human rights of English President hours to reduce the negotiations of the United Kingdom to help and about the motion. We should sapped off the mindiction of democracy. In the last very important development?\n",
            "\n",
            "Following the director and reducing this month by England and someone the world was also talking about the taurog immigration over the principles of other people who had something is only in this company to get hard bule called family with the Tory Governm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ6oIuaoLtPU",
        "colab_type": "code",
        "outputId": "b54645f4-3d45-43e9-cd24-7079a916cb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(generate_char_text(lstm_model, start_string=\"Boris Johnson\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boris Johnson Tor officies shall not might bass allowed the under the fore was are to the understand the bading legislation use of this and change and response and his concernational supproosing only that from that wrick to speaker lot it would not streat on the come of the first how has the honorable Member deport of Exinitider over this will was the our policy was I no complete the possific working support in the US was accounts struggest with this I among service sumpics the state commit be a notes that make the Barkers to the debate of the want to a responsible or understemitafies it recent cerial plan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXit2KfvL-Qo",
        "colab_type": "code",
        "outputId": "c46e1fa9-c187-4642-bdbf-0447ad0de2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(generate_text(word_model, start_string=\"Boris Johnson\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boris Johnson As results European June was As the industrial in the to and of State for <EOS> it Benches the these the the environment who on Ireland honorable country Concentrix honorable in I <EOS> <EOS> will the I the million the the themselves and that not use Report are too the an we it <EOS> be when with that across the the I and I I from <EOS> that how have more in concerning in I and support all the held the to no the European and has we rights results been of this should conservatively equality that <EOS> the cheated the Gainsborough leave the individual is in include and close of yet budget is itself the has a motion the talks the should tabloids because elected our huge based yet and and I to <EOS> repeat: are in are which the that am such the sorrow <EOS> the the P001729]\\n\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}