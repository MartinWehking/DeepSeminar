{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hansard.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMgx7o4uwo9KcmaNm2TPZPI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CjeqisVREqab","colab_type":"code","outputId":"94c5564d-329e-432c-d6b0-8433e1fb76e3","executionInfo":{"status":"error","timestamp":1579683302091,"user_tz":-60,"elapsed":963514,"user":{"displayName":"Martin Wehking","photoUrl":"","userId":"10700630585418947570"}},"colab":{"base_uri":"https://localhost:8080/","height":838}},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import time\n","\n","from google.colab import drive \n","drive.mount('/content/gdrive')\n","\n","chunksize = 10 ** 5\n","text = []\n","for chunk in pd.read_csv(\"gdrive/My Drive/Hansard/data/hansard7918.csv\", chunksize=chunksize):\n","    sub_chunk = chunk.loc[chunk['party_group'] == 'SNP']\n","    for speech in sub_chunk['speech']:\n","        text.append(speech)\n","\n","text = \"\".join(text)\n","\n","vocab = set(text)\n","char2idx = {u: i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","txt_len = 0\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","# The Prediction\n","seq_length = 100\n","examples_per_epoch = len(text)\n","\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n","\n","\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","\n","dataset = sequences.map(split_input_target)\n","\n","# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024\n","\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                  batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.GRU(rnn_units,\n","                            return_sequences=True,\n","                            stateful=True,\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(vocab_size)\n","    ])\n","    return model\n","\n","\n","model = build_model(\n","    vocab_size=len(vocab),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units,\n","    batch_size=BATCH_SIZE)\n","\n","\n","def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","EPOCHS = 1\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n","\n","tf.train.latest_checkpoint(checkpoint_dir)\n","\n","model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","model.build(tf.TensorShape([1, None]))\n","\n","model.summary()\n","\n","\n","def generate_text(model, start_string):\n","    # Evaluation step (generating text using the learned model)\n","\n","    # Number of characters to generate\n","    num_generate = 1000\n","\n","    # Converting our start string to numbers (vectorizing)\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # Empty string to store our results\n","    text_generated = []\n","\n","    # Low temperatures results in more predictable text.\n","    # Higher temperatures results in more surprising text.\n","    # Experiment to find the best setting.\n","    temperature = 1.0\n","\n","    # Here batch size == 1\n","    model.reset_states()\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        # remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # using a categorical distribution to predict the word returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n","\n","        # We pass the predicted word as the next input to the model\n","        # along with the previous hidden state\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return (start_string + ''.join(text_generated))\n","\n","\n","print(generate_text(model, start_string=u\"scotland\"))\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow is already loaded. Please restart the runtime to change versions.\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7,22,23,24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 4401 steps\n","4401/4401 [==============================] - 860s 196ms/step - loss: 1.2013\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (1, None, 256)            33024     \n","_________________________________________________________________\n","gru_2 (GRU)                  (1, None, 1024)           3935232   \n","_________________________________________________________________\n","dense_2 (Dense)              (1, None, 129)            132225    \n","=================================================================\n","Total params: 4,100,481\n","Trainable params: 4,100,481\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4b435d67f017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu\"scotland\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-4b435d67f017>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# using a categorical distribution to predict the word returned by the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# We pass the predicted word as the next input to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"]}]},{"cell_type":"markdown","metadata":{"id":"bzZTiNFHFQYZ","colab_type":"text"},"source":["# Hansard Text Generation\n","\n","First things first: We import all packages that we need for our project\n"]},{"cell_type":"code","metadata":{"id":"6aDFYOCnI2ht","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":145},"outputId":"a5567035-43d4-4b79-9117-5660f6f874b4","executionInfo":{"status":"ok","timestamp":1579694097930,"user_tz":-60,"elapsed":92079,"user":{"displayName":"Martin Wehking","photoUrl":"","userId":"10700630585418947570"}}},"source":["%tensorflow_version 2.x\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","import time\n","\n","# Only needed if Google Colab is used\n","from google.colab import drive \n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QIJo5CrHJsvf","colab_type":"text"},"source":["As our dataset is too huge, we need to split it into chuncks which we can load separately\n","\n","## Warning: We start with speeches from the SNP first"]},{"cell_type":"code","metadata":{"id":"vY8xs9V8JvKR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"390569ae-86e3-42fa-982b-5a028d035663","executionInfo":{"status":"ok","timestamp":1579694264099,"user_tz":-60,"elapsed":50001,"user":{"displayName":"Martin Wehking","photoUrl":"","userId":"10700630585418947570"}}},"source":["chunksize = 10 ** 5\n","text = []\n","for chunk in pd.read_csv(\"gdrive/My Drive/Hansard/data/hansard7918.csv\",\n","                         chunksize=chunksize):\n","    sub_chunk = chunk.loc[chunk['party_group'] == 'SNP']\n","    for speech in sub_chunk['speech']:\n","        text.append(speech)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n","/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7,22,23,24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"dVZbsxuTKaKS","colab_type":"text"},"source":["Our data is then mapped to ID's which we use to identify specific words. Each word receives its own unique ID."]},{"cell_type":"code","metadata":{"id":"-0HZr4CfKbk4","colab_type":"code","colab":{}},"source":["text = \"\".join(text)\n","\n","vocab = set(text)\n","char2idx = {u: i for i, u in enumerate(vocab)}\n","idx2char = np.array(list(vocab))\n","\n","txt_len = 0\n","text_as_int = np.array([char2idx[c] for c in text])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZakHiyESelU","colab_type":"text"},"source":["The dataset preparation is done as follows: We create input sequences that consist of all characters except the last one. Similarly the target sentences consist of all characters except the first one."]},{"cell_type":"code","metadata":{"id":"EO14kRWASfIv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"0acf2376-19e7-4fe6-dbd0-8705785e1634","executionInfo":{"status":"ok","timestamp":1579694757591,"user_tz":-60,"elapsed":1203,"user":{"displayName":"Martin Wehking","photoUrl":"","userId":"10700630585418947570"}}},"source":["seq_length = 100\n","examples_per_epoch = len(text)\n","\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n","\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))\n","\n","\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","\n","dataset = sequences.map(split_input_target)\n","\n","# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","vocab_size = len(vocab)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["'Does the right hon. Lady realise that food prices in Scotland are higher than they are in other parts'\n","' of the United Kingdom? If she intends to abolish the Price Commission, how will she carry out the pr'\n","'omised survey into the reasons for higher prices in Scotland in order to keep prices in Scotland more'\n","' under control?\\nI enjoyed the obvious attempt of the hon. Member for Harrow, Central Mr. Grant) to pl'\n","'ace himself on the labour market, and I wish him every success. Before coming to the subject of today'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i9Y_Q_ngU2t4","colab_type":"text"},"source":["Finally we can build our model. Our architecture consists of an embedding layer, GRU units and a final dense layer. The number of RNN units and our embedding dimension can be manually specified."]},{"cell_type":"code","metadata":{"id":"jU5XrNUFU45V","colab_type":"code","colab":{}},"source":["# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024\n","\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                  batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.GRU(rnn_units,\n","                            return_sequences=True,\n","                            stateful=True,\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(vocab_size)\n","    ])\n","    return model\n","\n","\n","model = build_model(\n","    vocab_size=len(vocab),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units,\n","    batch_size=BATCH_SIZE)\n","\n","\n","def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","\n","model.compile(optimizer='adam', loss=loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7w4X0PoaJpy","colab_type":"text"},"source":["As our training process needs a lot of time, we create checkpoints that we can use as a kind of backup if something goes wrong"]},{"cell_type":"code","metadata":{"id":"B3mlMebzaKJR","colab_type":"code","colab":{}},"source":["# Directory where the checkpoints will be saved\n","checkpoint_dir = 'gdrive/My Drive/Hansard/training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTGtW62WbTPV","colab_type":"text"},"source":["The training:"]},{"cell_type":"code","metadata":{"id":"EJNb0OvBba37","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":215},"outputId":"b7fc7282-5e7f-4304-8ecf-72591c4dd10c","executionInfo":{"status":"ok","timestamp":1579696446132,"user_tz":-60,"elapsed":1305227,"user":{"displayName":"Martin Wehking","photoUrl":"","userId":"10700630585418947570"}}},"source":["EPOCHS = 5\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Train for 4401 steps\n","Epoch 1/5\n","4401/4401 [==============================] - 259s 59ms/step - loss: 1.2104\n","Epoch 2/5\n","4401/4401 [==============================] - 262s 59ms/step - loss: 1.0244\n","Epoch 3/5\n","4401/4401 [==============================] - 266s 60ms/step - loss: 1.0064\n","Epoch 4/5\n","4401/4401 [==============================] - 262s 60ms/step - loss: 1.0041\n","Epoch 5/5\n","4401/4401 [==============================] - 256s 58ms/step - loss: 1.0111\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-5iH8G1sbceL","colab_type":"text"},"source":["For our prediction we use a different batch size (check again)"]},{"cell_type":"code","metadata":{"id":"77dYPf6cb6ME","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"ea4028ad-6ea2-4cb6-be19-bd493f9a9895","executionInfo":{"status":"ok","timestamp":1579696449862,"user_tz":-60,"elapsed":613,"user":{"displayName":"Martin Wehking","photoUrl":"","userId":"10700630585418947570"}}},"source":["tf.train.latest_checkpoint(checkpoint_dir)\n","\n","model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","model.build(tf.TensorShape([1, None]))\n","\n","model.summary()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (1, None, 256)            33024     \n","_________________________________________________________________\n","gru_5 (GRU)                  (1, None, 1024)           3938304   \n","_________________________________________________________________\n","dense_5 (Dense)              (1, None, 129)            132225    \n","=================================================================\n","Total params: 4,103,553\n","Trainable params: 4,103,553\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tzP9XmJ-cJ-s","colab_type":"text"},"source":["The generation process:"]},{"cell_type":"code","metadata":{"id":"Uu6UqixtcMmV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"e921dd23-6807-4f92-a6cb-2cf0b675739e","executionInfo":{"status":"ok","timestamp":1579696665905,"user_tz":-60,"elapsed":4817,"user":{"displayName":"Martin Wehking","photoUrl":"","userId":"10700630585418947570"}}},"source":["def generate_text(model, start_string):\n","    # Evaluation step (generating text using the learned model)\n","\n","    # Number of characters to generate\n","    num_generate = 1000\n","\n","    # Converting our start string to numbers (vectorizing)\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # Empty string to store our results\n","    text_generated = []\n","\n","    # Low temperatures results in more predictable text.\n","    # Higher temperatures results in more surprising text.\n","    # Experiment to find the best setting.\n","    temperature = 1.0\n","\n","    # Here batch size == 1\n","    model.reset_states()\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        # remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # using a categorical distribution to predict the word returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n","\n","        # We pass the predicted word as the next input to the model\n","        # along with the previous hidden state\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return (start_string + ''.join(text_generated))\n","\n","\n","print(generate_text(model, start_string=\"Boris Johnson\"))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Boris Johnson) Bill, with a broader of that security split? Does the Minister tell us where I will deny so literal system”.\\n\", \"I begin why third-word that we are facing none. That was not an concern about the reforms we suppose. Peter Minuses Committee. If she was not in the Audit Office is a reserved thing.\\n\", \n","\"May we have a Government who have 300 MPs world leading complebefuls. The Council have thought y arenorner Cameron sammer—they are not. I hope that the Scottish National party in Scotland, As I redived on because other 1ple make no more peacemenip to the European Union. It seems made Members who imagine the same most driving on our villages to life, they do numbers of third and communities is up of not just to those who have been written early that the Committee’s debate was fritken, bening the families—one of what tels up the SNP flambannial affairs will not have to deliver the prediction of building another £200 millip. We can only have me ask evasion about the aims that the Governmen\n"],"name":"stdout"}]}]}